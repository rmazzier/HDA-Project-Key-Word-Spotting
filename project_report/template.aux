\relax 
\providecommand\hyper@newdestlabel[2]{}
\AC@reset@newl@bel
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{dnns2014chen}
\citation{convnns2015sainath}
\citation{streamingkws2020Rybakov}
\citation{dnns2014chen}
\citation{deepreslearning2018tang}
\citation{convnns2015sainath}
\citation{mittermaier2020small}
\citation{choi2019temporal}
\citation{attentionisall2017vaswani}
\citation{vit2020Dosovitskiy}
\citation{touvron2021training}
\citation{gulati2020conformer}
\citation{kumar2021colorization}
\citation{Devlin2019BERTPO}
\citation{gdpr2017}
\citation{attention2018andreade}
\citation{attentionisall2017vaswani}
\citation{attentionisall2017vaswani}
\citation{mfccs1980davis}
\citation{dnns2014chen}
\citation{convnns2015sainath}
\citation{kim2021broadcasted}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{I}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{1}{section.2}\protected@file@percent }
\newlabel{sec:related_work}{{II}{1}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Foundations and state of the art}{1}{subsection.2.1}\protected@file@percent }
\citation{selfatt2016cheng}
\citation{luong2015effective}
\citation{attention2018andreade}
\citation{hochreiter1997long}
\citation{streamingkws2020Rybakov}
\citation{Cho2014gru}
\citation{streamingkws2020Rybakov}
\citation{kwtransformer2021berg}
\citation{vit2020Dosovitskiy}
\citation{speechdataset2018warden}
\citation{speechdataset2018warden}
\citation{speechdataset2018warden}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Overview of Att-RNN model (baseline) and SQAtt-RNN. The input $x$ is a matrix, where the $i$-th row is the vector of MFCCs computed for the $i$-th time frame. In both models, the CNN block outputs an image where the number of timesteps is preserved: feature vectors at each timestep are used as the input sequence for the RNN block. In the Att-RNN model, the attention layer returns a single context vector which is used for classification. In SQAtt-RNN, we use the whole sequence coming from the RNN block as query vectors: this results in a new sequence which is supposed to be a representation of the previous one. One last Bidirectional RNN layer scans the sequence and returns a single output vector, which is used for classification.\relax }}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:models_overview}{{1}{2}{Overview of Att-RNN model (baseline) and SQAtt-RNN. The input $x$ is a matrix, where the $i$-th row is the vector of MFCCs computed for the $i$-th time frame. In both models, the CNN block outputs an image where the number of timesteps is preserved: feature vectors at each timestep are used as the input sequence for the RNN block. In the Att-RNN model, the attention layer returns a single context vector which is used for classification. In SQAtt-RNN, we use the whole sequence coming from the RNN block as query vectors: this results in a new sequence which is supposed to be a representation of the previous one. One last Bidirectional RNN layer scans the sequence and returns a single output vector, which is used for classification.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}Models based on Attention}{2}{subsection.2.2}\protected@file@percent }
\citation{park2019specaugment}
\citation{Abadi2016TensorFlowAS}
\@writefile{toc}{\contentsline {section}{\numberline {III}Experimental Setup}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Signals and Features}{3}{subsection.3.1}\protected@file@percent }
\newlabel{sec:sig&features}{{\mbox  {III-A}}{3}{Signals and Features}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Input Pipeline}{3}{subsection.3.2}\protected@file@percent }
\newlabel{sec:processing_architecture}{{\mbox  {III-B}}{3}{Input Pipeline}{subsection.3.2}{}}
\citation{attention2018andreade}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Detailed description of the input pipeline for the training data. In the cache block, the data that was produced until that moment is saved to file. Each step which is performed before the caching operation happens only one time, during the first iteration of the dataset; on all the successive iterations, the data is read from the cached file. Boxes with the red outline denote data augmentation steps\relax }}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig:inputpipeline}{{2}{4}{Detailed description of the input pipeline for the training data. In the cache block, the data that was produced until that moment is saved to file. Each step which is performed before the caching operation happens only one time, during the first iteration of the dataset; on all the successive iterations, the data is read from the cached file. Boxes with the red outline denote data augmentation steps\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Learning Framework}{4}{subsection.3.3}\protected@file@percent }
\newlabel{sec:learning_framework}{{\mbox  {III-C}}{4}{Learning Framework}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-C}1}\textbf  {SimpleAtt}}{4}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Simple Attention RNN Architecture\relax }}{4}{table.caption.3}\protected@file@percent }
\newlabel{tab:simpleatt_architecture}{{1}{4}{Simple Attention RNN Architecture\relax }{table.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-C}2}\textbf  {Att-RNN}}{4}{subsubsection.3.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Att-RNN Architecture\relax }}{4}{table.caption.4}\protected@file@percent }
\newlabel{tab:attrnn_architecture}{{2}{4}{Att-RNN Architecture\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-C}3}\textbf  {SQAtt-RNN}}{4}{subsubsection.3.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces SQAtt-RNN Architecture\relax }}{4}{table.caption.5}\protected@file@percent }
\newlabel{tab:sqattrnn_architecture}{{3}{4}{SQAtt-RNN Architecture\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{4}{section.4}\protected@file@percent }
\newlabel{sec:results}{{IV}{4}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Attention Plots}{4}{subsection.4.1}\protected@file@percent }
\newlabel{fig:sub1}{{3a}{5}{A subfigure\relax }{figure.caption.6}{}}
\newlabel{sub@fig:sub1}{{a}{5}{A subfigure\relax }{figure.caption.6}{}}
\newlabel{fig:sub2}{{3b}{5}{A subfigure\relax }{figure.caption.6}{}}
\newlabel{sub@fig:sub2}{{b}{5}{A subfigure\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A figure with two subfigures\relax }}{5}{figure.caption.6}\protected@file@percent }
\newlabel{fig:accs_vs_parameters}{{3}{5}{A figure with two subfigures\relax }{figure.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Table with results\relax }}{5}{table.caption.7}\protected@file@percent }
\newlabel{table:results}{{4}{5}{Table with results\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Concluding Remarks}{5}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparison between attention scores from Att-RNN model (left) and MHAtt-RNN3 (right), on the words \textit  {off}, \textit  {yes} and \textit  {no}. \relax }}{5}{figure.caption.8}\protected@file@percent }
\newlabel{fig:att_scores}{{4}{5}{Comparison between attention scores from Att-RNN model (left) and MHAtt-RNN3 (right), on the words \textit {off}, \textit {yes} and \textit {no}. \relax }{figure.caption.8}{}}
\bibdata{biblio}
\bibcite{dnns2014chen}{{1}{}{{}}{{}}}
\bibcite{convnns2015sainath}{{2}{}{{}}{{}}}
\bibcite{streamingkws2020Rybakov}{{3}{}{{}}{{}}}
\bibcite{deepreslearning2018tang}{{4}{}{{}}{{}}}
\bibcite{mittermaier2020small}{{5}{}{{}}{{}}}
\bibcite{choi2019temporal}{{6}{}{{}}{{}}}
\bibcite{attentionisall2017vaswani}{{7}{}{{}}{{}}}
\bibcite{vit2020Dosovitskiy}{{8}{}{{}}{{}}}
\bibcite{touvron2021training}{{9}{}{{}}{{}}}
\bibcite{gulati2020conformer}{{10}{}{{}}{{}}}
\bibcite{kumar2021colorization}{{11}{}{{}}{{}}}
\bibcite{Devlin2019BERTPO}{{12}{}{{}}{{}}}
\bibcite{gdpr2017}{{13}{}{{}}{{}}}
\bibcite{attention2018andreade}{{14}{}{{}}{{}}}
\bibcite{mfccs1980davis}{{15}{}{{}}{{}}}
\bibcite{kim2021broadcasted}{{16}{}{{}}{{}}}
\bibcite{selfatt2016cheng}{{17}{}{{}}{{}}}
\bibcite{luong2015effective}{{18}{}{{}}{{}}}
\bibcite{hochreiter1997long}{{19}{}{{}}{{}}}
\bibcite{Cho2014gru}{{20}{}{{}}{{}}}
\bibcite{kwtransformer2021berg}{{21}{}{{}}{{}}}
\bibcite{speechdataset2018warden}{{22}{}{{}}{{}}}
\bibcite{park2019specaugment}{{23}{}{{}}{{}}}
\bibcite{Abadi2016TensorFlowAS}{{24}{}{{}}{{}}}
\bibstyle{ieeetr}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\newlabel{sec:conclusions}{{V}{6}{Concluding Remarks}{section.5}{}}
