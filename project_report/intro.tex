% !TEX root = template.tex

\section{Introduction}
\label{sec:introduction}
%Intro is here. Points to remember:
%\begin{itemize}
%	\item General short intro: why is this an interesting problem?
%	\item Go on more specifically hinting at what I have done;
%	\item Paper contributions: what is the problem, relevance, approach, value, applicability.
%	\item overview of the structure of the paper..
%	\item stay coincise: try to stay inside one page/one half
	
The Keyword Spotting (KWS) task consists in the detection of a certain predetermined set of keywords from a stream of user utterances. In recent years, this problem has become increasingly popular and, with the rapid development of mobile devices, it is now playing an important role in human-computer interaction, as well as encouraging the adoption of hands-free interfaces for a wide variety of use cases, that can range from \say{smart home} devices like Amazon Alexa, to virtual assistants such as Apple's Siri or Google Assistant. 
%KWS systems play a complementary role with respect to more complex, cloud based speech recognition services: indeed, their are required to run on local devices and are often used to initiate interactions by pronouncing the correct keyword. For this reason the research in KWS typically focuses on a tradeoff between highly precise predictions and small memory/computational footprint.

At present, Deep Learning techniques represent the state of the art approach for the KWS problem and have proved to give good results on tradeoff between model accuracy and memory footprint\cite{dnns2014chen} \cite{convnns2015sainath} \cite{streamingkws2020Rybakov}. Fully connected  and \MR{CNN based architectures have been widely explored, yielding good results} \cite{deepreslearning2018tang} \cite{convnns2015sainath} % TODO Aggiusta sta frase
\cite{mittermaier2020small} \cite{choi2019temporal}. In recent years, mostly thanks to the Transformer architecture introduced by Vaswani et al.~\cite{attentionisall2017vaswani}, interest towards attention-based architectures has grown dramatically. Indeed, recent works in machine learning have shown that models incorporating attention are able to provide groundbreaking results in a wide variety of domains \cite{vit2020Dosovitskiy} \cite{touvron2021training} \cite{gulati2020conformer} \cite{kumar2021colorization} \cite{Devlin2019BERTPO}. Another charming feature of attention-based systems is their inherent interpretability. Indeed, Explainable AI (XAI) is quickly becoming an hot topic in modern machine learning research, and the development of models capable of giving some sort of explanation for their predictions  will in time become more and more desirable, if not required \cite{gdpr2017}.

Motivated by those increasing trends, in this work we explore several applications of the attention mechanism for the KWS task. Specifically, we first explore an hybrid architecture, introduced by de Andreade et al. \cite{attention2018andreade}, constituted by a convolutional, a recurrent and an attention part. %TODO qui Ã¨ provvisorio!!
\MR{We explore the role of filter size for the convolutional block, to understand if vocal features can be extracted mostly from time or frequency domain.} We also propose some small adjustment to the attention layer and compare the performances with the more modern variant presented in \cite{streamingkws2020Rybakov}. We also examine the Keyword Transformer \cite{kwtransformer2021berg}, a recent architecture that exploits the strength of Transformers in a similar fashion to Vision Transformers \cite{vit2020Dosovitskiy}.

%one of the most explored classes of models is CNNs \cite{deepreslearning2018tang} \cite{convnns2015sainath} \cite{mittermaier2020small} \cite{choi2019temporal}, which are able to capture audio features both in the time and frequency domain, both from spectral and raw audio data. 

