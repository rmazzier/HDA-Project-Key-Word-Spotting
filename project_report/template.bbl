\begin{thebibliography}{10}

\bibitem{dnns2014chen}
G.~Chen, C.~Parada, and G.~Heigold, ``Small-footprint keyword spotting using
  deep neural networks,'' in {\em 2014 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, pp.~4087--4091, 2014.

\bibitem{convnns2015sainath}
T.~Sainath and C.~Parada, ``Convolutional neural networks for small-footprint
  keyword spotting,'' in {\em INTERSPEECH}, 2015.

\bibitem{streamingkws2020Rybakov}
O.~Rybakov, N.~Kononenko, N.~Subrahmanya, M.~Visontai, and S.~Laurenzo,
  ``Streaming keyword spotting on mobile devices,'' {\em Interspeech 2020}, Oct
  2020.

\bibitem{deepreslearning2018tang}
R.~Tang and J.~Lin, ``Deep residual learning for small-footprint keyword
  spotting,'' in {\em 2018 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP)}, pp.~5484--5488, IEEE, 2018.

\bibitem{mittermaier2020small}
S.~Mittermaier, L.~K{\"u}rzinger, B.~Waschneck, and G.~Rigoll,
  ``Small-footprint keyword spotting on raw audio data with
  sinc-convolutions,'' in {\em ICASSP 2020-2020 IEEE International Conference
  on Acoustics, Speech and Signal Processing (ICASSP)}, pp.~7454--7458, IEEE,
  2020.

\bibitem{choi2019temporal}
S.~Choi, S.~Seo, B.~Shin, H.~Byun, M.~Kersner, B.~Kim, D.~Kim, and S.~Ha,
  ``Temporal convolution for real-time keyword spotting on mobile devices,''
  {\em arXiv preprint arXiv:1904.03814}, 2019.

\bibitem{attentionisall2017vaswani}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in {\em
  Advances in neural information processing systems}, pp.~5998--6008, 2017.

\bibitem{vit2020Dosovitskiy}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, {\em et~al.},
  ``An image is worth 16x16 words: Transformers for image recognition at
  scale,'' {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{touvron2021training}
H.~Touvron, M.~Cord, M.~Douze, F.~Massa, A.~Sablayrolles, and H.~J{\'e}gou,
  ``Training data-efficient image transformers \& distillation through
  attention,'' in {\em International Conference on Machine Learning},
  pp.~10347--10357, PMLR, 2021.

\bibitem{gulati2020conformer}
A.~Gulati, J.~Qin, C.-C. Chiu, N.~Parmar, Y.~Zhang, J.~Yu, W.~Han, S.~Wang,
  Z.~Zhang, Y.~Wu, {\em et~al.}, ``Conformer: Convolution-augmented transformer
  for speech recognition,'' {\em arXiv preprint arXiv:2005.08100}, 2020.

\bibitem{kumar2021colorization}
M.~Kumar, D.~Weissenborn, and N.~Kalchbrenner, ``Colorization transformer,''
  {\em arXiv preprint arXiv:2102.04432}, 2021.

\bibitem{Devlin2019BERTPO}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' in {\em NAACL},
  2019.

\bibitem{gdpr2017}
B.~Goodman and S.~Flaxman, ``European union regulations on algorithmic
  decision-making and a `right to explanation`,'' {\em AI Magazine}, vol.~38,
  pp.~50--57, Oct 2017.

\bibitem{attention2018andreade}
D.~C. de~Andrade, S.~Leo, M.~L. D.~S. Viana, and C.~Bernkopf, ``A neural
  attention model for speech command recognition,'' {\em arXiv preprint
  arXiv:1808.08929}, 2018.

\bibitem{mfccs1980davis}
S.~Davis and P.~Mermelstein, ``Comparison of parametric representations for
  monosyllabic word recognition in continuously spoken sentences,'' {\em IEEE
  Transactions on Acoustics, Speech, and Signal Processing}, vol.~28, no.~4,
  pp.~357--366, 1980.

\bibitem{kim2021broadcasted}
B.~Kim, S.~Chang, J.~Lee, and D.~Sung, ``Broadcasted residual learning for
  efficient keyword spotting,'' {\em arXiv preprint arXiv:2106.04140}, 2021.

\bibitem{selfatt2016cheng}
J.~Cheng, L.~Dong, and M.~Lapata, ``Long short-term memory-networks for machine
  reading,'' {\em arXiv preprint arXiv:1601.06733}, 2016.

\bibitem{luong2015effective}
M.-T. Luong, H.~Pham, and C.~D. Manning, ``Effective approaches to
  attention-based neural machine translation,'' {\em arXiv preprint
  arXiv:1508.04025}, 2015.

\bibitem{hochreiter1997long}
S.~Hochreiter and J.~Schmidhuber, ``Long short-term memory,'' {\em Neural
  computation}, vol.~9, no.~8, pp.~1735--1780, 1997.

\bibitem{Cho2014gru}
K.~Cho, B.~V. Merrienboer, Çaglar G{\"u}lçehre, D.~Bahdanau, F.~Bougares,
  H.~Schwenk, and Y.~Bengio, ``Learning phrase representations using rnn
  encoder-decoder for statistical machine translation,'' in {\em EMNLP}, 2014.

\bibitem{kwtransformer2021berg}
A.~Berg, M.~O'Connor, and M.~T. Cruz, ``Keyword transformer: A self-attention
  model for keyword spotting,'' {\em arXiv preprint arXiv:2104.00769}, 2021.

\bibitem{speechdataset2018warden}
P.~Warden, ``Speech commands: A dataset for limited-vocabulary speech
  recognition,'' {\em arXiv preprint arXiv:1804.03209}, 2018.

\bibitem{park2019specaugment}
D.~S. Park, W.~Chan, Y.~Zhang, C.-C. Chiu, B.~Zoph, E.~D. Cubuk, and Q.~V. Le,
  ``Specaugment: A simple data augmentation method for automatic speech
  recognition,'' {\em arXiv preprint arXiv:1904.08779}, 2019.

\bibitem{Abadi2016TensorFlowAS}
M.~Abadi, P.~Barham, J.~Chen, Z.~Chen, A.~Davis, J.~Dean, M.~Devin,
  S.~Ghemawat, G.~Irving, M.~Isard, M.~Kudlur, J.~Levenberg, R.~Monga,
  S.~Moore, D.~Murray, B.~Steiner, P.~Tucker, V.~Vasudevan, P.~Warden,
  M.~Wicke, Y.~Yu, and X.~Zhang, ``Tensorflow: A system for large-scale machine
  learning,'' in {\em OSDI}, 2016.

\end{thebibliography}
