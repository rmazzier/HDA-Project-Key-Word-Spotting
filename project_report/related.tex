% !TEX root = template.tex

\section{Related Work}
\label{sec:related_work}

\subsection{Foundations and state of the art}

In recent years, machine learning techniques have proven to be the de-facto standard for approaching the KWS problem. Such models typically perform segmentation of the audio sequence on the time domain and extract log-mel scale spectrograms or mel-frequency cepstral coefficients (MFCC) \cite{mfccs1980davis} from each frame. Those are then used as input feature vectors for the models.
One of the first works exploring deep neural networks for the KWS task is from Chen et al \cite{dnns2014chen}: the authors explore small footprint fully connected architectures and show how they improved performances with respect to the baseline HMM models. This kind of architecture accounted for the sequentiality of audio data by stacking feature vectors of adjacent audio frames: while this gives good results compared to the baseline, it is a very simplistic way to model sequential data. Sainath and Parada~\cite{convnns2015sainath} approached the problem by using CNNs, a more fitting class of models which are able, by design, to capture several essential features of speech data (like input topology or translational invariance) and at the same time having a much smaller memory footprint due to parameter sharing. Indeed, CNNs have proven to be very successful for KWS: the current state of the art has recently been achieved in \cite{kim2021broadcasted}, where the authors introduce Broadcasted Residual Networks (BC-ResNets), a particular class of ResNets that are able to capture both 1D and 2D features thanks to the introduction of a new kind of residual block.

\subsection{Models based on Attention}
As a premise, it is useful to note that in this applications of attention, the query, key and value vectors are all represented by the feature vectors from the input sequence; therefore when referring to the attention mechanism, we will actually implicitly mean self-attention \cite{selfatt2016cheng}. Specifically, dot-product attention is adopted \cite{luong2015effective}.
In \cite{attention2018andreade}, De Andreade et al. propose an attention based convolutional recurrent neural network (Att-RNN), which takes as input a mel-scale spectrogram and extracts features in the frequency dimension with two convolutional layers. Then, such features are given as input to two bidirectional LSTM \cite{hochreiter1997long} layers, in order to extract long range dependencies from the inherently sequential audio data. The last\footnote{In the paper, the authors report to use the middle output, but looking at their implementation, they use the last one. Either way, as they point out, any output of the LSTM layer should work well as a query vector, since the bidirectional LSTM layer should be able to summarize the whole sequence in any of its outputs.} output feature from the LSTM layer is then transformed with a linear dense layer and used as a query vector for the attention mechanism. Finally, the sum of the LSTM outputs, weighted with respect to the attention scores, are given as input to a final dense MLP, which performs the classification. An immediate drawback of this approach is its impossibility to function in a streaming fashion (at least without introducing delay), since bidirectional recurrent layers require the whole sequence of data to work. Furthermore, by using a single vector representation as a query vector representative for the whole sequence, this could act as an information bottleneck. Nevertheless, the introduction of the attention mechanisms allows us to inspect the sections of audio which were responsible for the classification, increasing model transparency. In \cite{streamingkws2020Rybakov} the authors propose a more modern variant of the same architecture (referred as MHAtt-RNN): LSTM is substitued by GRU \cite{Cho2014gru}, and the attention mechanism is replaced by a ~\mbox{multi-head} attention layer using 4 heads. Despite increasing accuracy with respect to Att-RNN, this kind of models can be very heavy in terms of memory footprint, especially if using a high encoding dimension for the multi-head attention layer. In the case of \cite{streamingkws2020Rybakov}, the number of parameters is $743000$ which is more than four times the number of parameters of Att-RNN.

The latest contribution in attention based models for KWS comes from \cite{kwtransformer2021berg}, where the authors, inspired by the success of the newly introduced Vision Transformer (ViT) \cite{vit2020Dosovitskiy} for computer vision tasks, propose an adaptation of such architecture for keyword spotting, called the Keyword Transformer (KWT). The results were suprising: despite ViT proved to be competitive only when supported by pre-training on large datasets, KWT outperformed more complex models based on mixes between CNN, RNN and attention, even when trained on relatively small datasets, like the Google Speech Commands dataset \cite{speechdataset2018warden}. Even in this case, despite good performances, it might not be feasible to integrate the KWT model on small mobile devices: indeed, the best performing variation of the KWT  that was proposed is relying on a very deep architecture (12 Transformer encoder layers), which counts more than 5 million parameters.

By contrast, we propose lighter attention based architectures, taking Att-RNN as the baseline.
